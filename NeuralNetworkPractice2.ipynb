{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Network\n",
    "\n",
    "We know we need to set the number of input, hidden, and output layer nodes--and the learning rate. That defines the shape and size of the neural network. Rather than set these in stone, we'll let them be set when a new neural network object is created by using parameters. That way we retain the choice to create new neural networks of different sizes with ease.\n",
    "\n",
    "### Weights - The Heart of the Network\n",
    "\n",
    "The most important part of the network is the **link weights**. They're used to calculate the signal being fed forward, the error as it's propagated backwards, and it is the link weights themselves that are refined in an attempt to improve the network.\n",
    "\n",
    "We saw earlier that the weights can be concisely expressed as a matrix. So we can create:\n",
    "\n",
    "*    A matrix the weights for links between the input and hidden layers, $W_{input\\_hidden}$, of size (**hidden_nodes** by **input_nodes**)\n",
    "*   And another matrix for the links between the hidden and output layers, $W_{hidden\\_output}$, of size (**output_notes** by **hidden_nodes**)\n",
    "\n",
    "Remember the convention earlier to see why the first matrix is of size (**hidden_nodes** by **input_nodes** and not the other way around (**input_nodes** by **hidden_nodes**)\n",
    "\n",
    "Remember that the values of the link weights should be small and random. The following numpy function generates an array of values selected randomly between 0 and 1, where the size is (rows by columns): \n",
    "```\n",
    "numpy.random.rand(rows,columns)\n",
    "```\n",
    "\n",
    "### Querying the Network\n",
    "\n",
    "The query() function takes the input to a neural network and returns the network's output. That's simple enough, but to do that you'll remember that we need to pass the input signal from the input layer of nodes through the hidden layer and out of the final output layer. You'll also remember that we use the link weights to moderate the signals as they feed into any given hidden or output node, and we also use the sigmoid activation function to squish the signal coming out of those nodes.\n",
    "\n",
    "The following shows how the matrix of weights for the link between the input and hidden layers can be combined with the matrix of inputs to give the signals into the hidden layer nodes:\n",
    "\n",
    "$$X_{hidden} = W_{input\\_hidden} \\cdot I$$\n",
    "\n",
    "The following applies the numpy library's dot product function for matrices to the link weights $W_{input\\_hidden}$ and the inputs **I**:\n",
    "```\n",
    "hidden_inputs = numpy.dot(self.wih, inputs)\n",
    "```\n",
    "> This simple piece of Python code does all the work of combining all the inputs with all the right link weights to produce the matrix of combined moderated signals into each hidden layer node. We don't have to rewrite it either if next time we choose to use a different number of nodes for the input or hidden layer.\n",
    "\n",
    "To get the signals emerging from the hidden node, we simply apply the sigmoid squasing function to each of these emerging signals:\n",
    "\n",
    "$$O_{hidden} = sigmoid(X_{hidden})$$\n",
    "\n",
    "> The scipy Python library has a set of special functions, and the sigmoid function is called **expit()**\n",
    "\n",
    "Because we might want to experiment and tweak, or even completely change, the activation function, it makes sense to define it only *once* inside the neural network, when it is first intialized. After that, we can refer to it several times, such as in the query() function. This arrangement means we only need to change this definition once, and not have to locate and change the code anywhere an activation function is used.\n",
    "```\n",
    "self.activation_function = lambda x: scipy.special.expit(x)\n",
    "```\n",
    "\n",
    "> What is lambda? All we've done here is create a function with a shorter way of writing it out. Instead of the usual def() definitions, we use the magic lambda to create a function there and then (instance?).\n",
    ">> The function here takes x and returns scipy.special.expit(x) which is the sigmoid function. Functions created with lambda are nameless, or anonymous, but here we've assigned it the name self.activation_function(). All this means is that whenever someone needs to use the activation function, allt hey need to do is call self.activation_function().\n",
    "\n",
    "We want to apply the activation function to the combined and moderated signals into the hidden nodes. The signals emerging from thge hidden node layers are in the matrix called **hidden_outputs**.\n",
    "\n",
    "### Training the Network\n",
    "\n",
    "There are two phases to training:\n",
    "\n",
    "1.    Calculating the output, just as query() does it, for a given training example.\n",
    "    1. The only difference is that we have an additional parameter, **targets_list**, defined in the function name because you can't train the network without training examples which include the desired or target answer\n",
    "    2. The code also turns the **targets_list** into a numpy array, just as the **inputs_list** is turned into a numpy array\n",
    "2.    Take this calculated output, compare it with the desired output, and use the difference to guide the updating of the network weights. We use error backpropagation to inform thow the link weights are refined.\n",
    "    1. First, we need to calculate the error, which is the difference between the desired target output provided by the training example, and the actual calculated output.\n",
    "        1. That's the difference between the matrices (**targets** - **final_outputs**) done element by element\n",
    "        $$errors_{output} = targets - outputs_{final}$$\n",
    "    2. We can calculate the *back-propagated* errors for the hidden layer nodes. Remember how we split the errors according to the connected weights, and recombine them for each hidden layer node. We worked out the matrix form of this calculation as:\n",
    "    $$ errors_{hidden} = weights^{T}_{hidden\\_output} \\cdot errors_{output} $$\n",
    "\n",
    "Now we have what we need to refine the weights at each layer.\n",
    "1. For the weights between the hidden and final layers, we use the $errors_{output}$\n",
    "2. For the weights between the input and hidden layers, we use the $errors_{hidden}$\n",
    "\n",
    "We previously worked out the expression for updating the weight for the link between a node **j** and a node **k** in the next layer in matrix form:\n",
    "\n",
    "$$ \\Delta W_{jk} = \\alpha * E_k * sigmoid (O_k) * (1 - sigmoid (O_k)) \\cdot O^{T}_{j} $$\n",
    "\n",
    "*    The $\\alpha$ is the learning rate\n",
    "*    The **sigmoid** is the squashing activation function\n",
    "*    The $*$ is element by element matrix multiplication\n",
    "*    The $\\cdot$ dot is the matrix dot product\n",
    "*    The last bit, the matrix of outputs from the previous layer, is tranposed.\n",
    "    *     In effect, this means the column of outputs becomes a row of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neurral network class definition\n",
    "class neuralNetwork:\n",
    "    \n",
    "    # initialize the neural network\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n",
    "        # set number of nodes in each input, hidden, output layers\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # link weight matrixes, wih and who\n",
    "        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n",
    "        # w11 w21\n",
    "        # w12 w22 etc\n",
    "        \n",
    "        # We want to sample the weights from a normal probability distribution centered around zero\n",
    "        # with a standard deviation that is related to the number of incoming links to a node,\n",
    "        # 1 / sqrt(number of incoming links)\n",
    "        \n",
    "        self.wih = np.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # train the neural network\n",
    "    def train():\n",
    "        \n",
    "        #convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        #calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # error is the (target - actual)\n",
    "        output_errors = targets - final_outputs\n",
    "        \n",
    "        # hidden layer error is the output_errors, split by weights. recombined at hidden nodes\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        # update the weights for the links between the hidden and output layer\n",
    "        self.who += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "            # The learning rate is self.lr, and is simply multiplied with the rest of the expression\n",
    "            # There is matrix multiplication done by np.dot\n",
    "                # The two elements are\n",
    "                    # The error and sigmoids from the next layer\n",
    "                        # E_k * sigmoid(O_k) * (1-sigmoid(O_k))\n",
    "                    # The transposed outputs from the previous layer\n",
    "                        # O_j^T\n",
    "        \n",
    "        \n",
    "        # update the weights for the links between the input and hidden layers\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        \n",
    "        #convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signal into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the signals emerging from the hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate the signals into the final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        # calculate the signals emerging from the final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden, and output nodes\n",
    "input_nodes = 3\n",
    "hidden_nodes = 3\n",
    "output_nodes = 3\n",
    "\n",
    "# learning rate is 0.3\n",
    "learning_rate = 0.3\n",
    "\n",
    "# create instance of neural network\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56509363],\n",
       "       [0.45825764],\n",
       "       [0.48426891]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.query([1.0, 0.5, -1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-e8fc65e1b1ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGraphWin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphics'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
